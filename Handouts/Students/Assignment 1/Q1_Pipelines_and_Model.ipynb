{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Anmol Singhal\n",
    "Roll No: 11940140\n",
    "\n",
    "References:\n",
    "1. Learning Rate Schedule Taken From : https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "2. https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the SkLearn Pipeline for Data-Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"new-york-city-taxi-fare-prediction/train.csv\", nrows = 1000000)\n",
    "test_final = pd.read_csv(\"new-york-city-taxi-fare-prediction/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two base classes are used to make any normal python class a sklearn class.\n",
    "# \n",
    "# Every class inheriting these two classes must have to define atlease two mandatory functions\n",
    "#      1. fit(self, X, y=None) -> it only learns parameters & save those as class\n",
    "#                                 variables in the self object. as most of the time, it has nothing\n",
    "#                                 to return, so it returns the self object only\n",
    "#\n",
    "#      2. transform(self, X) -> it takes input values, apply some transformation like calculation of\n",
    "#                               prediction using learned parameters to make some changes in the input data.\n",
    "#                               and it retuens the updated input object \n",
    "#\n",
    "# there is another function as \"fit_transform()\", which calls \"fit()\" & \"transform()\" sequentially.\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline will Do the Following:\n",
    "\n",
    "1. Data Cleaning\n",
    "2. Adding New Features\n",
    "3. Feature Selection\n",
    "4. Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for Removing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class remove_missing_values(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.drop(X[X.isnull().any(1)].index, axis = 0)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for Removing Irrelevant Values from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class remove_irrelevant_values(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, isFareAmount = True,\n",
    "                 isPassengerCount = True,\n",
    "                 isLatitudes = True,\n",
    "                 isLongitudes = True):\n",
    "        self.isFareAmount = isFareAmount\n",
    "        self.isPassengerCount = isPassengerCount\n",
    "        self.isLatitudes = isLatitudes\n",
    "        self.isLongitudes = isLongitudes\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        if self.isFareAmount and 'fare_amount' in list(X.columns):\n",
    "            X = X.drop(X[X['fare_amount']<=0].index, axis=0)\n",
    "            \n",
    "        if self.isPassengerCount:\n",
    "            X = X.drop(X[X['passenger_count']<=0].index, axis=0)\n",
    "            X = X.drop(X[X['passenger_count']>=7].index, axis=0)\n",
    "        \n",
    "        if self.isLatitudes:\n",
    "            X = X.drop(X[X['pickup_latitude']<-90].index, axis=0)\n",
    "            X = X.drop(X[X['pickup_latitude']>90].index, axis=0)\n",
    "\n",
    "            X = X.drop(X[X['dropoff_latitude']<-90].index, axis=0)\n",
    "            X = X.drop(X[X['dropoff_latitude']>90].index, axis=0)\n",
    "        \n",
    "        if self.isLongitudes:\n",
    "            X = X.drop(X[X['pickup_longitude']<-180].index, axis=0)\n",
    "            X = X.drop(X[X['pickup_longitude']>180].index, axis=0)\n",
    "\n",
    "            X = X.drop(X[X['dropoff_longitude']<-180].index, axis=0)\n",
    "            X = X.drop(X[X['dropoff_longitude']>180].index, axis=0)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to Remove Rows with Locations outside of a Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class remove_outside_bb(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, bb = (-74.5, -72.8, 40.5, 41.8)):\n",
    "        self.bb = bb\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        self.outlier_index = X.loc[(X['pickup_longitude'] < self.bb[0]) | (X['pickup_longitude'] > self.bb[1]) | \\\n",
    "           (X['pickup_latitude'] < self.bb[2]) | (X['pickup_latitude'] > self.bb[3]) | \\\n",
    "           (X['dropoff_longitude'] < self.bb[0]) | (X['dropoff_longitude'] > self.bb[1]) | \\\n",
    "           (X['dropoff_latitude'] < self.bb[2]) | (X['dropoff_latitude'] > self.bb[3])].index\n",
    "        X = X.drop(self.outlier_index, axis=0)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to Fix Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fix_datetime_data_types(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, cols = ['pickup_datetime', 'key']):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        for col in self.cols:\n",
    "            X[col] = pd.to_datetime(X[col])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to Add Date Time based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_datetime_features(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, dt_col='pickup_datetime',\n",
    "                 isHour = False,\n",
    "                 isDayOfWeek = False,\n",
    "                 isDayOfMonth = False,\n",
    "                 isWeekOfYear = False,\n",
    "                 isMonth = False,\n",
    "                 isYear = True):\n",
    "        self.dt_col = dt_col\n",
    "        self.isHour = isHour\n",
    "        self.isDayOfWeek = isDayOfWeek\n",
    "        self.isDayOfMonth = isDayOfMonth\n",
    "        self.isWeekOfYear = isWeekOfYear\n",
    "        self.isMonth = isMonth\n",
    "        self.isYear = isYear\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.isHour:\n",
    "            X[\"hour\"] = X[self.dt_col].dt.hour\n",
    "        if self.isDayOfWeek:\n",
    "            X[\"day_of_week\"] = X[self.dt_col].dt.weekday\n",
    "        if self.isDayOfMonth:\n",
    "            X[\"day_of_month\"] = X[self.dt_col].dt.day\n",
    "        if self.isWeekOfYear:\n",
    "            X[\"week\"] = X[self.dt_col].dt.isocalendar().week\n",
    "        if self.isMonth:\n",
    "            X[\"month\"] = X[self.dt_col].dt.month\n",
    "        if self.isYear:\n",
    "            X[\"year\"] = X[self.dt_col].dt.year        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to remove Irrelevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class remove_features(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, cols = ['pickup_datetime', 'key', 'passenger_count', 'pickup_latitude', 'dropoff_latitude']):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        for col in self.cols:\n",
    "            X = X.drop(col, axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to Add New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_features(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, isHaversine=True,\n",
    "                isChebyshev=False,\n",
    "                \n",
    "                isDistFromJFK_pickup=True,\n",
    "                isDistFromJFK_dropoff=True,\n",
    "                jfk_coords = (40.639722, -73.778889),\n",
    "                \n",
    "                isDistFromEWR_pickup=False,\n",
    "                isDistFromEWR_dropoff=False,\n",
    "                ewr_coords = (40.6925, -74.168611),\n",
    "                \n",
    "                isDistFromLGA_pickup=False,\n",
    "                isDistFromLGA_dropoff=False,\n",
    "                lga_coords = (40.77725, -73.872611),\n",
    "                \n",
    "                isDistFromSOL_pickup=False,\n",
    "                isDistFromSOL_dropoff=True,\n",
    "                sol_coords = (40.689247, -74.044502),\n",
    "                \n",
    "                isDistFromESM_pickup=False,\n",
    "                isDistFromESM_dropoff=False,\n",
    "                esm_coords = (40.748440, -73.985664),\n",
    "                \n",
    "                isDistFromCP_pickup=True,\n",
    "                isDistFromCP_dropoff=True,\n",
    "                cp_coords = (40.782864, -73.965355),\n",
    "                \n",
    "                isDistFromTS_pickup=False,\n",
    "                isDistFromTS_dropoff=False,\n",
    "                ts_coords = (40.759010, -73.984474),\n",
    "                \n",
    "                isNearJFK_pickup=True,\n",
    "                isNearJFK_dropoff=True,\n",
    "                \n",
    "                isNearEWR_pickup=False,\n",
    "                isNearEWR_dropoff=True,\n",
    "\n",
    "                isNearLGA_pickup=True,\n",
    "                isNearLGA_dropoff=True,\n",
    "                \n",
    "                isNearSOL_pickup=False,\n",
    "                isNearSOL_dropoff=False,\n",
    "                \n",
    "                isNearESM_pickup=True,\n",
    "                isNearESM_dropoff=True,\n",
    "                \n",
    "                isNearCP_pickup=False,\n",
    "                isNearCP_dropoff=False,\n",
    "                \n",
    "                isNearTS_pickup=True,\n",
    "                isNearTS_dropoff=True,\n",
    "\n",
    "                ):\n",
    "    \n",
    "        self.isHaversine = isHaversine\n",
    "        self.isChebyshev = isChebyshev\n",
    "\n",
    "        self.isDistFromJFK_pickup = isDistFromJFK_pickup\n",
    "        self.isDistFromJFK_dropoff = isDistFromJFK_dropoff\n",
    "        self.jfk_coords = jfk_coords\n",
    "\n",
    "        self.isDistFromEWR_pickup = isDistFromEWR_pickup\n",
    "        self.isDistFromEWR_dropoff = isDistFromEWR_dropoff\n",
    "        self.ewr_coords = ewr_coords\n",
    "\n",
    "        self.isDistFromLGA_pickup = isDistFromLGA_pickup\n",
    "        self.isDistFromLGA_dropoff = isDistFromLGA_dropoff\n",
    "        self.lga_coords = lga_coords\n",
    "        \n",
    "        self.isDistFromSOL_pickup = isDistFromSOL_pickup\n",
    "        self.isDistFromSOL_dropoff = isDistFromSOL_dropoff\n",
    "        self.sol_coords = sol_coords\n",
    "        \n",
    "        self.isDistFromESM_pickup = isDistFromESM_pickup\n",
    "        self.isDistFromESM_dropoff = isDistFromESM_dropoff\n",
    "        self.esm_coords = esm_coords\n",
    "        \n",
    "        self.isDistFromCP_pickup = isDistFromCP_pickup\n",
    "        self.isDistFromCP_dropoff = isDistFromCP_dropoff\n",
    "        self.cp_coords = cp_coords\n",
    "        \n",
    "        self.isDistFromTS_pickup = isDistFromTS_pickup\n",
    "        self.isDistFromTS_dropoff = isDistFromTS_dropoff\n",
    "        self.ts_coords = ts_coords\n",
    "\n",
    "        self.isNearJFK_pickup = isNearJFK_pickup\n",
    "        self.isNearJFK_dropoff = isNearJFK_dropoff\n",
    "\n",
    "        self.isNearEWR_pickup = isNearEWR_pickup\n",
    "        self.isNearEWR_dropoff = isNearEWR_dropoff\n",
    "\n",
    "        self.isNearLGA_pickup = isNearLGA_pickup\n",
    "        self.isNearLGA_dropoff = isNearLGA_dropoff\n",
    "        \n",
    "        self.isNearSOL_pickup = isNearSOL_pickup\n",
    "        self.isNearSOL_dropoff = isNearSOL_dropoff\n",
    "        \n",
    "        self.isNearESM_pickup = isNearESM_pickup\n",
    "        self.isNearESM_dropoff = isNearESM_dropoff\n",
    "        \n",
    "        self.isNearCP_pickup = isNearCP_pickup\n",
    "        self.isNearCP_dropoff = isNearCP_dropoff\n",
    "        \n",
    "        self.isNearTS_pickup = isNearTS_pickup\n",
    "        self.isNearTS_dropoff = isNearTS_dropoff\n",
    "\n",
    "    def add_haversine_dist_from(self, dataset, airport_coordinates, lat_col, long_col, target_name):\n",
    "            \n",
    "        R = 6371  #radius of earth in kilometers\n",
    "        #R = 3959 #radius of earth in miles\n",
    "        phi1 = np.radians(dataset[lat_col])\n",
    "        phi2 = np.radians(airport_coordinates[0])\n",
    "\n",
    "        delta_phi = np.radians(airport_coordinates[0] - dataset[lat_col])\n",
    "        delta_lambda = np.radians(airport_coordinates[1] - dataset[long_col])\n",
    "\n",
    "        #a = sin²((φB - φA)/2) + cos φA . cos φB . sin²((λB - λA)/2)\n",
    "        a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n",
    "\n",
    "        #c = 2 * atan2( √a, √(1−a) )\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "\n",
    "        #d = R*c\n",
    "        d = (R * c) #in kilometers\n",
    "        dataset[target_name] = d\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        if self.isHaversine:\n",
    "            \n",
    "            R = 6371  #radius of earth in kilometers\n",
    "            #R = 3959 #radius of earth in miles\n",
    "            phi1 = np.radians(X['pickup_latitude'])\n",
    "            phi2 = np.radians(X['dropoff_latitude'])\n",
    "\n",
    "            delta_phi = np.radians(X['dropoff_latitude']-X['pickup_latitude'])\n",
    "            delta_lambda = np.radians(X['dropoff_longitude']-X['pickup_longitude'])\n",
    "\n",
    "            #a = sin²((φB - φA)/2) + cos φA . cos φB . sin²((λB - λA)/2)\n",
    "            a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n",
    "\n",
    "            #c = 2 * atan2( √a, √(1−a) )\n",
    "            c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "\n",
    "            #d = R*c\n",
    "            d = (R * c) #in kilometers\n",
    "            X['H_Distance'] = d\n",
    "        \n",
    "        if self.isChebyshev:\n",
    "            X['Chebyshev'] = np.maximum(np.absolute(X['dropoff_longitude'] - X['pickup_longitude']), np.absolute(X['dropoff_latitude'] - X['pickup_latitude']))\n",
    "        \n",
    "        if self.isDistFromJFK_pickup:\n",
    "            self.add_haversine_dist_from(X, self.jfk_coords, 'pickup_latitude', 'pickup_longitude', 'jfk_Distance_Pickup')\n",
    "        if self.isDistFromJFK_dropoff:\n",
    "            self.add_haversine_dist_from(X, self.jfk_coords, 'dropoff_latitude', 'dropoff_longitude', 'jfk_Distance_Dropoff')\n",
    "        \n",
    "        if self.isDistFromEWR_pickup:\n",
    "            self.add_haversine_dist_from(X, self.ewr_coords, 'pickup_latitude', 'pickup_longitude', 'ewr_Distance_Pickup')\n",
    "        if self.isDistFromEWR_dropoff:\n",
    "            self.add_haversine_dist_from(X, self.ewr_coords, 'dropoff_latitude', 'dropoff_longitude', 'ewr_Distance_Dropoff')\n",
    "            \n",
    "        if self.isDistFromLGA_pickup:\n",
    "            self.add_haversine_dist_from(X, self.lga_coords, 'pickup_latitude', 'pickup_longitude', 'lga_Distance_Pickup')\n",
    "        if self.isDistFromLGA_dropoff:\n",
    "            self.add_haversine_dist_from(X, self.lga_coords, 'dropoff_latitude', 'dropoff_longitude', 'lga_Distance_Dropoff')\n",
    "        \n",
    "        if self.isDistFromSOL_pickup:\n",
    "            self.add_haversine_dist_from(X, self.sol_coords, 'pickup_latitude', 'pickup_longitude', 'sol_Distance_Pickup')\n",
    "        if self.isDistFromSOL_dropoff:\n",
    "            self.add_haversine_dist_from(X, self.sol_coords, 'dropoff_latitude', 'dropoff_longitude', 'sol_Distance_Dropoff')\n",
    "            \n",
    "        if self.isDistFromESM_pickup:\n",
    "            self.add_haversine_dist_from(X, self.esm_coords, 'pickup_latitude', 'pickup_longitude', 'esm_Distance_Pickup')\n",
    "        if self.isDistFromESM_dropoff:\n",
    "            self.add_haversine_dist_from(X, self.esm_coords, 'dropoff_latitude', 'dropoff_longitude', 'esm_Distance_Dropoff')\n",
    "        \n",
    "        if self.isDistFromCP_pickup:\n",
    "            self.add_haversine_dist_from(X, self.cp_coords, 'pickup_latitude', 'pickup_longitude', 'cp_Distance_Pickup')\n",
    "        if self.isDistFromCP_dropoff:\n",
    "            self.add_haversine_dist_from(X, self.cp_coords, 'dropoff_latitude', 'dropoff_longitude', 'cp_Distance_Dropoff')\n",
    "        \n",
    "        if self.isDistFromTS_pickup:\n",
    "            self.add_haversine_dist_from(X, self.ts_coords, 'pickup_latitude', 'pickup_longitude', 'ts_Distance_Pickup')\n",
    "        if self.isDistFromTS_dropoff:\n",
    "            self.add_haversine_dist_from(X, self.ts_coords, 'dropoff_latitude', 'dropoff_longitude', 'ts_Distance_Dropoff')\n",
    "        \n",
    "        # Considering 4 KM Distance\n",
    "        DISTANCE_THRESHOLD = 4\n",
    "        \n",
    "        if self.isNearJFK_pickup:\n",
    "            if 'jfk_Distance_Pickup' in list(X.columns):\n",
    "                X['pickup_near_jfk'] = X['jfk_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from JFK to each row\n",
    "                self.add_haversine_dist_from(X, self.jfk_coords, 'pickup_latitude', 'pickup_longitude', 'jfk_Distance_Pickup')\n",
    "                # Now add Near JFK column\n",
    "                X['pickup_near_jfk'] = X['jfk_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from JFK column\n",
    "                X.drop(['jfk_Distance_Pickup'], axis=1, inplace=True)\n",
    "        \n",
    "        if self.isNearJFK_dropoff:\n",
    "            if 'jfk_Distance_Dropoff' in list(X.columns):\n",
    "                X['dropoff_near_jfk'] = X['jfk_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from JFK to each row\n",
    "                self.add_haversine_dist_from(X, self.jfk_coords, 'dropoff_latitude', 'dropoff_longitude', 'jfk_Distance_Dropoff')\n",
    "                # Now add Near JFK column\n",
    "                X['dropoff_near_jfk'] = X['jfk_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from JFK column\n",
    "                X.drop(['jfk_Distance_Dropoff'], axis=1, inplace=True)\n",
    "        \n",
    "        if self.isNearEWR_pickup:\n",
    "            if 'ewr_Distance_Pickup' in list(X.columns):\n",
    "                X['pickup_near_ewr'] = X['ewr_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from EWR to each row\n",
    "                self.add_haversine_dist_from(X, self.ewr_coords, 'pickup_latitude', 'pickup_longitude', 'ewr_Distance_Pickup')\n",
    "                # Now add Near EWR column\n",
    "                X['pickup_near_ewr'] = X['ewr_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from EWR column\n",
    "                X.drop(['ewr_Distance_Pickup'], axis=1, inplace=True)\n",
    "            \n",
    "        if self.isNearEWR_dropoff:\n",
    "            if 'ewr_Distance_Dropoff' in list(X.columns):\n",
    "                X['dropoff_near_ewr'] = X['ewr_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from EWR to each row\n",
    "                self.add_haversine_dist_from(X, self.ewr_coords, 'dropoff_latitude', 'dropoff_longitude', 'ewr_Distance_Dropoff')\n",
    "                # Now add Near EWR column\n",
    "                X['dropoff_near_ewr'] = X['ewr_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from EWR column\n",
    "                X.drop(['ewr_Distance_Dropoff'], axis=1, inplace=True)\n",
    "            \n",
    "        if self.isNearLGA_pickup:\n",
    "            if 'lga_Distance_Pickup' in list(X.columns):\n",
    "                X['pickup_near_lga'] = X['lga_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from LGA to each row\n",
    "                self.add_haversine_dist_from(X, self.lga_coords, 'pickup_latitude', 'pickup_longitude', 'lga_Distance_Pickup')\n",
    "                # Now add Near LGA column\n",
    "                X['pickup_near_lga'] = X['lga_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from LGA column\n",
    "                X.drop(['lga_Distance_Pickup'], axis=1, inplace=True)\n",
    "        \n",
    "        if self.isNearLGA_dropoff:\n",
    "            if 'lga_Distance_Dropoff' in list(X.columns):\n",
    "                X['dropoff_near_lga'] = X['lga_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from LGA to each row\n",
    "                self.add_haversine_dist_from(X, self.lga_coords, 'dropoff_latitude', 'dropoff_longitude', 'lga_Distance_Dropoff')\n",
    "                # Now add Near LGA column\n",
    "                X['dropoff_near_lga'] = X['lga_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from LGA column\n",
    "                X.drop(['lga_Distance_Dropoff'], axis=1, inplace=True)\n",
    "        \n",
    "        if self.isNearSOL_pickup:\n",
    "            if 'sol_Distance_Pickup' in list(X.columns):\n",
    "                X['pickup_near_sol'] = X['sol_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from SOL to each row\n",
    "                self.add_haversine_dist_from(X, self.sol_coords, 'pickup_latitude', 'pickup_longitude', 'sol_Distance_Pickup')\n",
    "                # Now add Near SOL column\n",
    "                X['pickup_near_sol'] = X['sol_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from SOL column\n",
    "                X.drop(['sol_Distance_Pickup'], axis=1, inplace=True)\n",
    "        \n",
    "        if self.isNearSOL_dropoff:\n",
    "            if 'sol_Distance_Dropoff' in list(X.columns):\n",
    "                X['dropoff_near_sol'] = X['sol_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from SOL to each row\n",
    "                self.add_haversine_dist_from(X, self.sol_coords, 'dropoff_latitude', 'dropoff_longitude', 'sol_Distance_Dropoff')\n",
    "                # Now add Near SOL column\n",
    "                X['dropoff_near_sol'] = X['sol_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from SOL column\n",
    "                X.drop(['sol_Distance_Dropoff'], axis=1, inplace=True)\n",
    "        \n",
    "        if self.isNearESM_pickup:\n",
    "            if 'esm_Distance_Pickup' in list(X.columns):\n",
    "                X['pickup_near_esm'] = X['esm_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from ESM to each row\n",
    "                self.add_haversine_dist_from(X, self.esm_coords, 'pickup_latitude', 'pickup_longitude', 'esm_Distance_Pickup')\n",
    "                # Now add Near ESM column\n",
    "                X['pickup_near_esm'] = X['esm_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from ESM column\n",
    "                X.drop(['esm_Distance_Pickup'], axis=1, inplace=True)\n",
    "        \n",
    "        if self.isNearESM_dropoff:\n",
    "            if 'esm_Distance_Dropoff' in list(X.columns):\n",
    "                X['dropoff_near_esm'] = X['esm_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from ESM to each row\n",
    "                self.add_haversine_dist_from(X, self.esm_coords, 'dropoff_latitude', 'dropoff_longitude', 'esm_Distance_Dropoff')\n",
    "                # Now add Near ESM column\n",
    "                X['dropoff_near_esm'] = X['esm_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from ESM column\n",
    "                X.drop(['esm_Distance_Dropoff'], axis=1, inplace=True)\n",
    "        \n",
    "        if self.isNearCP_pickup:\n",
    "            if 'cp_Distance_Pickup' in list(X.columns):\n",
    "                X['pickup_near_cp'] = X['cp_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from CP to each row\n",
    "                self.add_haversine_dist_from(X, self.cp_coords, 'pickup_latitude', 'pickup_longitude', 'cp_Distance_Pickup')\n",
    "                # Now add Near CP column\n",
    "                X['pickup_near_cp'] = X['cp_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from CP column\n",
    "                X.drop(['cp_Distance_Pickup'], axis=1, inplace=True)\n",
    "        \n",
    "        if self.isNearCP_dropoff:\n",
    "            if 'cp_Distance_Dropoff' in list(X.columns):\n",
    "                X['dropoff_near_cp'] = X['cp_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from CP to each row\n",
    "                self.add_haversine_dist_from(X, self.cp_coords, 'dropoff_latitude', 'dropoff_longitude', 'cp_Distance_Dropoff')\n",
    "                # Now add Near CP column\n",
    "                X['dropoff_near_cp'] = X['cp_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from CP column\n",
    "                X.drop(['cp_Distance_Dropoff'], axis=1, inplace=True)\n",
    "        \n",
    "        if self.isNearTS_pickup:\n",
    "            if 'ts_Distance_Pickup' in list(X.columns):\n",
    "                X['pickup_near_ts'] = X['ts_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from TS to each row\n",
    "                self.add_haversine_dist_from(X, self.ts_coords, 'pickup_latitude', 'pickup_longitude', 'ts_Distance_Pickup')\n",
    "                # Now add Near TS column\n",
    "                X['pickup_near_ts'] = X['ts_Distance_Pickup'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from TS column\n",
    "                X.drop(['ts_Distance_Pickup'], axis=1, inplace=True)\n",
    "        \n",
    "        if self.isNearTS_dropoff:\n",
    "            if 'ts_Distance_Dropoff' in list(X.columns):\n",
    "                X['dropoff_near_ts'] = X['ts_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "            else:\n",
    "                # First Add Distance from TS to each row\n",
    "                self.add_haversine_dist_from(X, self.ts_coords, 'dropoff_latitude', 'dropoff_longitude', 'ts_Distance_Dropoff')\n",
    "                # Now add Near TS column\n",
    "                X['dropoff_near_ts'] = X['ts_Distance_Dropoff'].apply(lambda x: 1 if x < DISTANCE_THRESHOLD else 0)\n",
    "                # Remove Distance from TS column\n",
    "                X.drop(['ts_Distance_Dropoff'], axis=1, inplace=True)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to Impute Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class impute_features(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, isHaversine=True):\n",
    "        self.isHaversine = isHaversine\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "\n",
    "        if self.isHaversine:\n",
    "            X.loc[X['H_Distance'] == 0, 'H_Distance'] = (X['fare_amount'] - 2.50) / 1.56\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to DownCast Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class downcast_numeric_features(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_int = X.select_dtypes(include=['int64', 'int32', 'int16', 'int8', 'int'])\n",
    "        X[X_int.columns] = X_int.apply(pd.to_numeric,downcast='unsigned')\n",
    "        X_float = X.select_dtypes(include=['float64', 'float32', 'float16', 'float'])\n",
    "        X[X_float.columns] = X_float.apply(pd.to_numeric,downcast='float')\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to Scale the Data and Return the Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class Scaler(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler = StandardScaler()\n",
    "        # Pipeline is being fit to the dataframe with the target column in it\n",
    "        if 'fare_amount' in list(X.columns):\n",
    "            self.scaler.fit(X.drop(['fare_amount'], axis=1))\n",
    "        else:\n",
    "            # Target Column is not in the dataframe\n",
    "            self.scaler.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(\"Current Columns:\", X.columns)\n",
    "        # We need to perform Scaling on Dataframe but not on fare_amount\n",
    "        if 'fare_amount' in list(X.columns):\n",
    "            y = X['fare_amount'].to_numpy().reshape(-1,1)\n",
    "            X = X.drop(['fare_amount'], axis=1)\n",
    "            X = self.scaler.transform(X)\n",
    "            X = np.hstack((X, y))\n",
    "            return X\n",
    "        else:\n",
    "            return self.scaler.transform(X)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to add Ones Column to the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_ones(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Add a column of ones to the numpy array as the first column\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Pipeline for Data-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline object to apply the above class in sequence\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pre_pipeline = Pipeline([\n",
    "    ('remove missing values', remove_missing_values()),\n",
    "    ('remove irrelevant values', remove_irrelevant_values()),\n",
    "    ('remove outside NYC', remove_outside_bb()),\n",
    "    ('fix datetime data types', fix_datetime_data_types()),\n",
    "    ('add datetime features', add_datetime_features()),\n",
    "    ('add features', add_features()),\n",
    "    ('remove extra features', remove_features()),\n",
    "    ('impute features', impute_features()),\n",
    "    ('downcast numeric features', downcast_numeric_features()),\n",
    "    ('StandardScaler', Scaler()),\n",
    "    ('Add Ones', add_ones())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data into Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split original dataset into 80% train and 20% test\n",
    "train_data = data.sample(frac=0.8, random_state=42)\n",
    "test_data = data.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Pre-Processing Pipeline to the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the Pipeline to Train Data and Transform the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Columns: Index(['fare_amount', 'pickup_longitude', 'dropoff_longitude', 'year',\n",
      "       'H_Distance', 'jfk_Distance_Pickup', 'jfk_Distance_Dropoff',\n",
      "       'sol_Distance_Dropoff', 'cp_Distance_Pickup', 'cp_Distance_Dropoff',\n",
      "       'pickup_near_jfk', 'dropoff_near_jfk', 'dropoff_near_ewr',\n",
      "       'pickup_near_lga', 'dropoff_near_lga', 'pickup_near_esm',\n",
      "       'dropoff_near_esm', 'pickup_near_ts', 'dropoff_near_ts'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_data = pre_pipeline.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Fare\\_Amount$ Column is in our Data, so it is shifted to the last and is not Scaled by the Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[:,:-1]\n",
    "y_train = train_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((780465, 19), (780465,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Columns: Index(['fare_amount', 'pickup_longitude', 'dropoff_longitude', 'year',\n",
      "       'H_Distance', 'jfk_Distance_Pickup', 'jfk_Distance_Dropoff',\n",
      "       'sol_Distance_Dropoff', 'cp_Distance_Pickup', 'cp_Distance_Dropoff',\n",
      "       'pickup_near_jfk', 'dropoff_near_jfk', 'dropoff_near_ewr',\n",
      "       'pickup_near_lga', 'dropoff_near_lga', 'pickup_near_esm',\n",
      "       'dropoff_near_esm', 'pickup_near_ts', 'dropoff_near_ts'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "test_data = pre_pipeline.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[:,:-1]\n",
    "y_test = test_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((195047, 19), (195047,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Fold Cross Validation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For manual Implementation of Cross Validation, I have implemented the **Repeated Random Sub Sampling CV** method.\n",
    "\n",
    "It is more robust than the K-fold standard method.\n",
    "\n",
    "Reference: [Neptune AI](https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MSE, R2, MAE, RMSE from sklearn.metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# Import Train Test Split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Manual Implementation of Cross Validation\n",
    "def cross_validate(model, X, y, cv=5, isEarlyStoppingUsingVal=False):\n",
    "    rmse = []\n",
    "    mae = []\n",
    "    r2 = []\n",
    "    for i in range(cv):\n",
    "        # Split X and y into train and Validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "        if not isEarlyStoppingUsingVal:\n",
    "            # Fit the model to the train data\n",
    "            model.fit(X_train, y_train)\n",
    "        else:\n",
    "            # Fit the model but also pass in the validation data for early stopping (This is Optimization Based Model)\n",
    "            model.fit(X_train, y_train, X_val, y_val)\n",
    "        # Predict on the validation data\n",
    "        y_pred = model.predict(X_val)\n",
    "        # Calculate the score of the model on the validation data\n",
    "        rmse.append(np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "        mae.append(mean_absolute_error(y_val, y_pred))\n",
    "        r2.append(r2_score(y_val, y_pred))\n",
    "    return {'rmse': np.array(rmse), 'mae': np.array(mae), 'r2': np.array(r2)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Beautifully Display the Ouput of Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(scores, rmse = True, mae = True, r2 = True):\n",
    "    rmse_scores = scores['rmse']\n",
    "    mae_scores = scores['mae']\n",
    "    r2_scores = scores['r2']\n",
    "    \n",
    "    if rmse:\n",
    "        # Print RMSE Scores\n",
    "        print(\"*************************\")\n",
    "        print('RMSE Scores:')\n",
    "        print(\"*************************\")\n",
    "        print(\"Scores: {}\".format(rmse_scores))\n",
    "        print(\"Mean: {}\".format(rmse_scores.mean()))\n",
    "        print(\"Std: {}\".format(rmse_scores.std()))\n",
    "        print(\"*************************\")\n",
    "        print()\n",
    "\n",
    "    if mae:\n",
    "        # Print MAE Scores\n",
    "        print(\"*************************\")\n",
    "        print('MAE Scores:')\n",
    "        print(\"*************************\")\n",
    "        print(\"Scores: {}\".format(mae_scores))\n",
    "        print(\"Mean: {}\".format(mae_scores.mean()))\n",
    "        print(\"Std: {}\".format(mae_scores.std()))\n",
    "        print(\"*************************\")\n",
    "        print()\n",
    "    \n",
    "    if r2:\n",
    "        # Print R2 Scores\n",
    "        print(\"*************************\")\n",
    "        print('R2 Scores:')\n",
    "        print(\"*************************\")\n",
    "        print(\"Scores: {}\".format(r2_scores))\n",
    "        print(\"Mean: {}\".format(r2_scores.mean()))\n",
    "        print(\"Std: {}\".format(r2_scores.std()))\n",
    "        print(\"*************************\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable to store all scores of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Trying Matrix Based Linear Regression Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i)    $w_{estimate} = (A^{T}A)^{-1}A^{T}y_{real}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.w_estimate = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        self.n = X.shape[0]\n",
    "        self.d = X.shape[1]\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.d != X.shape[1]:\n",
    "            raise ValueError('Dimension of X does not match dimension of model expected: {}'.format((self.d)))\n",
    "        return X @ self.w_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validate This Approach on the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_linear_regression = SimpleLinearRegression()\n",
    "slr_scores = cross_validate(simple_linear_regression, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "RMSE Scores:\n",
      "*************************\n",
      "Scores: [5.04330889 4.81932008 4.58737578 4.77733321 4.71654854 4.94277339\n",
      " 4.82211913 4.97569076 4.66094894 4.82120803]\n",
      "Mean: 4.816662676483373\n",
      "Std: 0.134539487583856\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_scores(slr_scores, rmse=True, mae=False, r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores['Simple Linear Regression'] = slr_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) $w_{estimate} = A^{⊕}y_{real}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoInverseLinearRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Pseudo Inverse of A\n",
    "        self.w_estimate = np.linalg.pinv(X) @ y\n",
    "        self.n = X.shape[0]\n",
    "        self.d = X.shape[1]\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.d != X.shape[1]:\n",
    "            raise ValueError('Dimension of X does not match dimension of model expected: {}'.format((self.d)))\n",
    "        return X @ self.w_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validate This Approach on the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_linear_regression = PseudoInverseLinearRegression()\n",
    "plr_scores = cross_validate(pseudo_linear_regression, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "RMSE Scores:\n",
      "*************************\n",
      "Scores: [5.04330889 4.81932008 4.58737578 4.77733321 4.71654854 4.94277339\n",
      " 4.82211913 4.97569076 4.66094894 4.82120803]\n",
      "Mean: 4.816662676483373\n",
      "Std: 0.1345394875838561\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_scores(plr_scores, rmse=True, mae=False, r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores['Pseudo Inverse Linear Regression'] = plr_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) $w_{estimate} = R^{-1}Q^{T}y_{real}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRLinearRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Q, R = np.linalg.qr(X)\n",
    "        self.w_estimate = np.linalg.inv(R) @ Q.T @ y\n",
    "        self.n = X.shape[0]\n",
    "        self.d = X.shape[1]\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.d != X.shape[1]:\n",
    "            raise ValueError('Dimension of X does not match dimension of model expected: {}'.format((self.d)))\n",
    "        return X @ self.w_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validate This Approach on the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_regression = QRLinearRegression()\n",
    "qr_scores = cross_validate(qr_regression, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "RMSE Scores:\n",
      "*************************\n",
      "Scores: [5.04330889 4.81932008 4.58737578 4.77733321 4.71654854 4.94277339\n",
      " 4.82211913 4.97569076 4.66094894 4.82120803]\n",
      "Mean: 4.816662676483373\n",
      "Std: 0.13453948758385617\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_scores(qr_scores, rmse=True, mae=False, r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores['QR Linear Regression'] = qr_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) Simple Linear Regression using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validate This Approach on the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_slr_regression = LinearRegression()\n",
    "sk_slr_scores = cross_validate(sk_slr_regression, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "RMSE Scores:\n",
      "*************************\n",
      "Scores: [5.04330889 4.81932008 4.58737578 4.77733321 4.71654854 4.94277339\n",
      " 4.82211913 4.97569076 4.66094894 4.82120803]\n",
      "Mean: 4.816662676483373\n",
      "Std: 0.13453948758385603\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_scores(sk_slr_scores, rmse=True, mae=False, r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores['Sklearn Simple Linear Regression'] = sk_slr_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (v) Ridge Regression using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will Tune the Hyper-Parameter $\\lambda$ for Ridge Regression\n",
    "\n",
    "Where $\\lambda$ is the Regularization Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.0001, 0.001,0.01, 0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different solvers for the Ridge Regression, but we will stick with $cholesky$ for now as we are considering matrix based solutions and it is the one taught in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = 'cholesky'\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyper-Parameter Tuning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_tuning_scores = {}\n",
    "for l in lambdas:\n",
    "    sk_ridge_regression = Ridge(alpha=l, solver=solver, random_state=seed)\n",
    "    sk_ridge_scores = cross_validate(sk_ridge_regression, X_train, y_train, cv=10)\n",
    "    l_tuning_scores[l] = sk_ridge_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Scores for each lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda Value: 0.0001\n",
      "Mean RMSE Score: 4.816662676471172\n",
      "Std. in RMSE Score: 0.13453948756921277\n",
      "Lambda Value: 0.001\n",
      "Mean RMSE Score: 4.816662676361256\n",
      "Std. in RMSE Score: 0.13453948743744293\n",
      "Lambda Value: 0.01\n",
      "Mean RMSE Score: 4.81666267526208\n",
      "Std. in RMSE Score: 0.13453948611974564\n",
      "Lambda Value: 0.1\n",
      "Mean RMSE Score: 4.816662664270679\n",
      "Std. in RMSE Score: 0.13453947294276966\n",
      "Lambda Value: 1\n",
      "Mean RMSE Score: 4.816662554391599\n",
      "Std. in RMSE Score: 0.13453934117286698\n",
      "Lambda Value: 10\n",
      "Mean RMSE Score: 4.816661459092447\n",
      "Std. in RMSE Score: 0.13453802345935117\n"
     ]
    }
   ],
   "source": [
    "for l in lambdas:\n",
    "    print(\"Lambda Value:\", l)\n",
    "    print(\"Mean RMSE Score:\", l_tuning_scores[l]['rmse'].mean())\n",
    "    print(\"Std. in RMSE Score:\", l_tuning_scores[l]['rmse'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores for each $\\lambda$ seem to be pretty much the same, so we will stick with $\\lambda = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_ridge_scores = l_tuning_scores[1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "RMSE Scores:\n",
      "*************************\n",
      "Scores: [5.04330843 4.81931909 4.58737613 4.77733358 4.71654882 4.94277357\n",
      " 4.82211893 4.97569041 4.66094851 4.82120809]\n",
      "Mean: 4.816662554391599\n",
      "Std: 0.13453934117286698\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_scores(sk_ridge_scores, rmse=True, mae=False, r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores['Sklearn Ridge Regression'] = sk_ridge_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (vi) Lasso Regression using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will Tune the Hyper-Parameter $\\lambda$ for Lasso Regression\n",
    "\n",
    "Where $\\lambda$ is the Regularization Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.001, 0.01, 0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyper-Parameter Tuning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 10\n",
      "Lambda: 1\n",
      "Lambda: 0.1\n",
      "Lambda: 0.01\n",
      "Lambda: 0.001\n"
     ]
    }
   ],
   "source": [
    "l_tuning_scores_lasso = {}\n",
    "for l in lambdas[::-1]:\n",
    "    print(\"Lambda:\", l)\n",
    "    sk_lasso_regression = Lasso(alpha=l, max_iter=10000, random_state=seed)\n",
    "    sk_lasso_scores = cross_validate(sk_lasso_regression, X_train, y_train, cv=10)\n",
    "    l_tuning_scores_lasso[l] = sk_lasso_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Scores for each lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda Value: 0.001\n",
      "Mean RMSE Score: 4.816701142384302\n",
      "Std. in RMSE Score: 0.1345366670079097\n",
      "Lambda Value: 0.01\n",
      "Mean RMSE Score: 4.81763826703508\n",
      "Std. in RMSE Score: 0.13450774871143759\n",
      "Lambda Value: 0.1\n",
      "Mean RMSE Score: 4.869981452707974\n",
      "Std. in RMSE Score: 0.1359601122825223\n",
      "Lambda Value: 1\n",
      "Mean RMSE Score: 5.41208578733203\n",
      "Std. in RMSE Score: 0.1376848113477227\n",
      "Lambda Value: 10\n",
      "Mean RMSE Score: 9.75093243901511\n",
      "Std. in RMSE Score: 0.07505701007643602\n"
     ]
    }
   ],
   "source": [
    "for l in lambdas:\n",
    "    print(\"Lambda Value:\", l)\n",
    "    print(\"Mean RMSE Score:\", l_tuning_scores_lasso[l]['rmse'].mean())\n",
    "    print(\"Std. in RMSE Score:\", l_tuning_scores_lasso[l]['rmse'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores for each $\\lambda$ seem to increase as we decrease the $\\lambda$. We will stick with $\\lambda = 0.001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_lasso_scores = l_tuning_scores_lasso[0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "RMSE Scores:\n",
      "*************************\n",
      "Scores: [5.04323604 4.81915949 4.58744961 4.77733444 4.71682149 4.94291219\n",
      " 4.82215445 4.97575332 4.66076459 4.82142579]\n",
      "Mean: 4.816701142384302\n",
      "Std: 0.1345366670079097\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_scores(sk_lasso_scores, rmse=True, mae=False, r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores['Sklearn Lasso Regression'] = sk_lasso_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (vii) Elastic Net Regression using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Elastic Net, we will Tune two Hyper-Parameters:\n",
    "\n",
    "1. $\\lambda$ i.e. Regularization Parameter\n",
    "2. $ll\\_ratio$ i.e. Mix Ratio of $L1$ and $L2$ Regularization\n",
    "\n",
    "$ll\\_ratio = 1$ means Lasso Regression i.e. only $L1$ Regularization <br>\n",
    "$ll\\_ratio = 0$ means Ridge Regression i.e. only $L2$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.001, 0.01, 0.1, 1, 10]\n",
    "ll_ratios = [0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 45 Combinations of $\\lambda$ and $ll\\_ratio$ are considered for the Hyper-Parameter Tuning\n",
    "\n",
    "This step takes lots of time, so we will use `Randomized Grid Search` to find the best Hyper-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented **Randomized Grid** Search to find the best Hyper-Parameters\n",
    "\n",
    "- We will try 30 different Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.) Lambda: 0.01 L1 Ratio: 0.7\n",
      "1.) Lambda: 0.1 L1 Ratio: 0.1\n",
      "2.) Lambda: 0.001 L1 Ratio: 0.7\n",
      "3.) Lambda: 10 L1 Ratio: 0.7\n",
      "4.) Lambda: 1 L1 Ratio: 0.4\n",
      "5.) Lambda: 0.1 L1 Ratio: 0.7\n",
      "6.) Lambda: 0.001 L1 Ratio: 0.2\n",
      "7.) Lambda: 1 L1 Ratio: 0.7\n",
      "8.) Lambda: 1 L1 Ratio: 0.5\n",
      "9.) Lambda: 0.1 L1 Ratio: 0.4\n",
      "10.) Lambda: 0.01 L1 Ratio: 0.8\n",
      "11.) Lambda: 1 L1 Ratio: 0.1\n",
      "12.) Lambda: 0.01 L1 Ratio: 0.4\n",
      "13.) Lambda: 1 L1 Ratio: 0.8\n",
      "14.) Lambda: 1 L1 Ratio: 0.3\n",
      "15.) Lambda: 0.001 L1 Ratio: 0.5\n",
      "16.) Lambda: 10 L1 Ratio: 0.8\n",
      "17.) Lambda: 0.1 L1 Ratio: 0.3\n",
      "18.) Lambda: 10 L1 Ratio: 0.2\n",
      "19.) Lambda: 0.001 L1 Ratio: 0.4\n",
      "20.) Lambda: 0.01 L1 Ratio: 0.2\n",
      "21.) Lambda: 0.1 L1 Ratio: 0.5\n",
      "22.) Lambda: 0.001 L1 Ratio: 0.9\n",
      "23.) Lambda: 0.01 L1 Ratio: 0.5\n",
      "24.) Lambda: 10 L1 Ratio: 0.4\n",
      "25.) Lambda: 1 L1 Ratio: 0.9\n",
      "26.) Lambda: 0.001 L1 Ratio: 0.1\n",
      "27.) Lambda: 0.01 L1 Ratio: 0.3\n",
      "28.) Lambda: 10 L1 Ratio: 0.3\n",
      "29.) Lambda: 0.1 L1 Ratio: 0.8\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "l_tuning_scores_elastic = {}\n",
    "\n",
    "for _ in range(30):\n",
    "    \n",
    "    l = random.choice(lambdas)\n",
    "    ll = random.choice(ll_ratios)\n",
    "    while (l, ll) in l_tuning_scores_elastic:\n",
    "        # If a combination is already visited, choose another one\n",
    "        l = random.choice(lambdas)\n",
    "        ll = random.choice(ll_ratios)\n",
    "        \n",
    "    print(str(_)+\".)\", \"Lambda:\", l, \"L1 Ratio:\", ll)\n",
    "    sk_elastic_regression = ElasticNet(alpha=l, l1_ratio=ll, max_iter=10000, random_state=seed)\n",
    "    sk_elastic_scores = cross_validate(sk_elastic_regression, X_train, y_train, cv=10)\n",
    "    l_tuning_scores_elastic[(l, ll)] = sk_elastic_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the best parameters out of these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will select the combination with the lowest Average RMSE Score\n",
    "min_avg_rmse = float('inf')\n",
    "std_rmse = float('inf')\n",
    "min_avg_rmse_l = None\n",
    "\n",
    "for (l, ll) in l_tuning_scores_elastic:\n",
    "    if l_tuning_scores_elastic[(l, ll)]['rmse'].mean() < min_avg_rmse:\n",
    "        min_avg_rmse = l_tuning_scores_elastic[(l, ll)]['rmse'].mean()\n",
    "        std_rmse = l_tuning_scores_elastic[(l, ll)]['rmse'].std()\n",
    "        min_avg_rmse_l = (l, ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Combination is: (0.001, 0.1) with an Average RMSE Score of: 4.81661063548957 and a Std. Dev. of: 0.13445685286865294\n"
     ]
    }
   ],
   "source": [
    "print(\"The Best Combination is:\", min_avg_rmse_l, \"with an Average RMSE Score of:\", min_avg_rmse, \"and a Std. Dev. of:\", std_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_elastic_scores = l_tuning_scores_elastic[min_avg_rmse_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "RMSE Scores:\n",
      "*************************\n",
      "Scores: [5.0430546  4.81876002 4.5875908  4.7775493  4.71674455 4.94289333\n",
      " 4.82203123 4.97550987 4.66069899 4.82127366]\n",
      "Mean: 4.81661063548957\n",
      "Std: 0.13445685286865294\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_scores(sk_elastic_scores, rmse=True, mae=False, r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores['Sklearn Elastic Net Regression'] = sk_elastic_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (viii) Manually Implemented Ridge Regression (Closed Form Solution)\n",
    "\n",
    "### $w_{estimate} = (A^{T}A + \\lambda I_{d+1})^{-1}A^{T}y_{real}$\n",
    "\n",
    "Where $A$ is the Input Matrix with ones column added, $y_{real}$ is the Real Output, $\\lambda$ is the Regularization Parameter and $I_{d+1}$ is the Identity Matrix with $d+1$ rows and $d+1$ columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression:\n",
    "    \n",
    "    def __init__(self, lambda_=0.1):\n",
    "        self.lambda_ = lambda_\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.n = X.shape[0]\n",
    "        self.d = X.shape[1]\n",
    "        self.w_estimate = np.linalg.inv(X.T @ X + self.lambda_ * np.eye(self.d)) @ X.T @ y\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.d != X.shape[1]:\n",
    "            raise ValueError('Dimension of X does not match dimension of model expected: {}'.format((self.d)))\n",
    "        return X @ self.w_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will Tune the Hyper-Parameter $\\lambda$ for Ridge Regression\n",
    "\n",
    "Where $\\lambda$ is the Regularization Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.0001, 0.001,0.01, 0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyper-Parameter Tuning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_tuning_scores = {}\n",
    "for l in lambdas:\n",
    "    ridge_regression = RidgeRegression(lambda_=l)\n",
    "    ridge_scores = cross_validate(ridge_regression, X_train, y_train, cv=10)\n",
    "    ridge_tuning_scores[l] = ridge_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Scores for each lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda Value: 0.0001\n",
      "Mean RMSE Score: 4.816662676471172\n",
      "Std. in RMSE Score: 0.13453948756921277\n",
      "Lambda Value: 0.001\n",
      "Mean RMSE Score: 4.816662676361256\n",
      "Std. in RMSE Score: 0.13453948743744293\n",
      "Lambda Value: 0.01\n",
      "Mean RMSE Score: 4.81666267526208\n",
      "Std. in RMSE Score: 0.13453948611974564\n",
      "Lambda Value: 0.1\n",
      "Mean RMSE Score: 4.816662664270679\n",
      "Std. in RMSE Score: 0.13453947294276966\n",
      "Lambda Value: 1\n",
      "Mean RMSE Score: 4.816662554391599\n",
      "Std. in RMSE Score: 0.13453934117286698\n",
      "Lambda Value: 10\n",
      "Mean RMSE Score: 4.816661459092447\n",
      "Std. in RMSE Score: 0.13453802345935117\n"
     ]
    }
   ],
   "source": [
    "for l in lambdas:\n",
    "    print(\"Lambda Value:\", l)\n",
    "    print(\"Mean RMSE Score:\", l_tuning_scores[l]['rmse'].mean())\n",
    "    print(\"Std. in RMSE Score:\", l_tuning_scores[l]['rmse'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores for each $\\lambda$ seem to be pretty much the same, so we will stick with $\\lambda = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_scores = l_tuning_scores[1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "RMSE Scores:\n",
      "*************************\n",
      "Scores: [5.04330843 4.81931909 4.58737613 4.77733358 4.71654882 4.94277357\n",
      " 4.82211893 4.97569041 4.66094851 4.82120809]\n",
      "Mean: 4.816662554391599\n",
      "Std: 0.13453934117286698\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_scores(ridge_scores, rmse=True, mae=False, r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores['Ridge Regression Manual'] = ridge_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Net Accuracies so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('total_scores_11940140.pickle', 'wb') as f:\n",
    "#     pickle.dump(total_scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Trying Optimization Based Linear Regression Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SGD](image_sgd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\theta_{new} = \\theta - \\eta \\times \\nabla_{\\theta}MSE(\\theta)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class BatchGradientRegressor:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, n_iters=1000, seed=42, error_threshold=1e-6, convergence_threshold=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.error_threshold = error_threshold\n",
    "        self.convergence_threshold = convergence_threshold\n",
    "        self.seed = seed\n",
    "    \n",
    "    def calculate_gradient(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Calculate the gradient of the loss function with respect to w.\n",
    "        \"\"\"\n",
    "        return (2 / len(y)) * X.T @ (X @ w - y)\n",
    "    \n",
    "    # X_val and y_val are the validation sets used for early stopping\n",
    "    def fit(self, X, y, X_val, y_val):\n",
    "        \n",
    "        self.d = X.shape[1]\n",
    "        \n",
    "        # Randomise an Initial Theta with seed = self.seed\n",
    "        np.random.seed(self.seed)\n",
    "        theta = np.random.randn(self.d)\n",
    "        \n",
    "        # Iterate\n",
    "        for _ in range(self.n_iters):\n",
    "            \n",
    "            # Calculate the Gradient\n",
    "            grad = self.calculate_gradient(X, y, theta)\n",
    "            # Calculate New Theta\n",
    "            theta_new = theta - self.learning_rate * grad\n",
    "            \n",
    "            # Check Convergence\n",
    "            if np.linalg.norm(theta_new - theta) < self.convergence_threshold:\n",
    "                theta = theta_new\n",
    "                break\n",
    "            \n",
    "            # Check MSE for Early Stopping\n",
    "            y_pred = X_val @ theta_new\n",
    "            mse = mean_squared_error(y_pred, y_val)\n",
    "            if mse < self.error_threshold:\n",
    "                theta = theta_new\n",
    "                break\n",
    "            \n",
    "            # Update Theta\n",
    "            theta = theta_new.copy()\n",
    "            \n",
    "        self.theta = theta.copy()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the output of a dataset X.\n",
    "        \"\"\"\n",
    "        if X.shape[1] != self.d:\n",
    "            raise ValueError(\"X has wrong number of features, expected {}, got {}.\".format(self.d, X.shape[1]))\n",
    "        return X @ self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will Tune the parameter $\\eta$ for Batch Gradient Descent\n",
    "\n",
    "Where $\\eta$ is the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_values = [0.01, 0.1, 0.5, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta Value: 0.01\n",
      "Eta Value: 0.1\n",
      "Eta Value: 0.5\n",
      "Eta Value: 1\n",
      "Eta Value: 2\n"
     ]
    }
   ],
   "source": [
    "b_tuning_scores_batch = {}\n",
    "\n",
    "for eta in eta_values:\n",
    "    print(\"Eta Value:\", eta)\n",
    "    batch_gradient_regression = BatchGradientRegressor(learning_rate=eta, n_iters=100, seed=42)\n",
    "    batch_gradient_scores = cross_validate(batch_gradient_regression, X_train, y_train, cv=10, isEarlyStoppingUsingVal=True)\n",
    "    b_tuning_scores_batch[eta] = batch_gradient_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the best value for $\\eta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rmse = float('inf')\n",
    "min_rmse_eta = None\n",
    "min_rmse_std = None\n",
    "\n",
    "for eta in b_tuning_scores_batch:\n",
    "    if b_tuning_scores_batch[eta]['rmse'].mean() < min_rmse:\n",
    "        min_rmse = b_tuning_scores_batch[eta]['rmse'].mean()\n",
    "        min_rmse_eta = eta\n",
    "        min_rmse_std = b_tuning_scores_batch[eta]['rmse'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Eta Value is: 0.1 with an Average RMSE Score of: 4.815670541368664 and a Std. Dev. of: 0.13473601004127464\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Eta Value is:\", min_rmse_eta, \"with an Average RMSE Score of:\", min_rmse, \"and a Std. Dev. of:\", min_rmse_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "RMSE Scores:\n",
      "*************************\n",
      "Scores: [5.0414098  4.82100321 4.58776729 4.77704287 4.71286819 4.94371966\n",
      " 4.81358413 4.97697486 4.66045338 4.82188203]\n",
      "Mean: 4.815670541368664\n",
      "Std: 0.13473601004127464\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_gradient_regression_scores = b_tuning_scores_batch[min_rmse_eta]\n",
    "print_scores(batch_gradient_regression_scores, rmse=True, mae=False, r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores['Batch Gradient Regression'] = batch_gradient_regression_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class StochasticGradientRegressor:\n",
    "    \n",
    "    def __init__(self, initial_learning_rate=0.1, n_epochs=100, seed=42, error_threshold=1e-6, convergence_threshold=1e-9, k=0.1):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.error_threshold = error_threshold\n",
    "        self.convergence_threshold = convergence_threshold\n",
    "        self.k = k\n",
    "        self.seed = seed\n",
    "    \n",
    "    def calculate_gradient(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Calculate the gradient of the loss function with respect to w.\n",
    "        \"\"\"\n",
    "        return (2 / len(y)) * X.T @ (X @ w - y)\n",
    "    \n",
    "    def exponential_learning_schedule(self, t):\n",
    "        \"\"\"\n",
    "        Exponential Learning Schedule.\n",
    "        \"\"\"\n",
    "        return self.initial_learning_rate * np.exp(-t * self.k)\n",
    "    \n",
    "    # X_val and y_val are the validation sets used for early stopping\n",
    "    def fit(self, X, y, X_val, y_val):\n",
    "        \n",
    "        self.d = X.shape[1]\n",
    "        \n",
    "        # Randomise an Initial Theta with seed = self.seed\n",
    "        np.random.seed(self.seed)\n",
    "        theta = np.random.randn(self.d)\n",
    "        \n",
    "        # Iterate\n",
    "        t = 0\n",
    "        flag = False\n",
    "        for _ in range(self.n_epochs):\n",
    "            \n",
    "            for i in range(len(y)):\n",
    "                \n",
    "                # Calculate the Gradient\n",
    "                grad = self.calculate_gradient(np.array([X[i]]), np.array([y[i]]), theta)\n",
    "                # Calculate New Theta\n",
    "                theta_new = theta - self.exponential_learning_schedule(t=t) * grad\n",
    "                \n",
    "                # Check Convergence\n",
    "                if np.linalg.norm(theta_new - theta) < self.convergence_threshold:\n",
    "                    theta = theta_new\n",
    "                    flag = True\n",
    "                    break\n",
    "                \n",
    "                # Update Theta\n",
    "                theta = theta_new.copy()\n",
    "                t += 1\n",
    "            \n",
    "            # Check MSE for Early Stopping after each epoch\n",
    "            y_pred = X_val @ theta\n",
    "            mse = mean_squared_error(y_pred, y_val)\n",
    "            if mse < self.error_threshold:\n",
    "                theta = theta_new\n",
    "                flag = True\n",
    "                break\n",
    "            \n",
    "            # Checking for Convergence or Early Stopping\n",
    "            if flag:\n",
    "                break\n",
    "            \n",
    "        self.theta = theta.copy()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the output of a dataset X.\n",
    "        \"\"\"\n",
    "        if X.shape[1] != self.d:\n",
    "            raise ValueError(\"X has wrong number of features, expected {}, got {}.\".format(self.d, X.shape[1]))\n",
    "        return X @ self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Above Gradient Descent, We have selected an Exponential Decay Function for Learning Rate $\\eta$\n",
    "\n",
    "$$\\eta_{new} = \\eta_{initial} \\times \\exp(-k * t)$$\n",
    "\n",
    "where $k$ is the Learning Rate Decay Factor and $t$ is the Current Iteration Number\n",
    "\n",
    "Hence there are two hyper-parameters to tune:\n",
    "\n",
    "1. $\\eta$ i.e. Learning Rate\n",
    "2. $k$ i.e. Decay Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_values = [0.01, 0.1, 0.5, 1, 2, 5]\n",
    "k_values = [0.1, 0.5, 1, 2, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Total of 30 Combinations of $\\eta$ and $k$ are considered for the Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta Value: 0.01 K Value: 0.1\n",
      "Eta Value: 0.01 K Value: 0.5\n",
      "Eta Value: 0.01 K Value: 1\n",
      "Eta Value: 0.01 K Value: 2\n",
      "Eta Value: 0.01 K Value: 5\n",
      "Eta Value: 0.1 K Value: 0.1\n",
      "Eta Value: 0.1 K Value: 0.5\n",
      "Eta Value: 0.1 K Value: 1\n",
      "Eta Value: 0.1 K Value: 2\n",
      "Eta Value: 0.1 K Value: 5\n",
      "Eta Value: 0.5 K Value: 0.1\n",
      "Eta Value: 0.5 K Value: 0.5\n",
      "Eta Value: 0.5 K Value: 1\n",
      "Eta Value: 0.5 K Value: 2\n",
      "Eta Value: 0.5 K Value: 5\n",
      "Eta Value: 1 K Value: 0.1\n",
      "Eta Value: 1 K Value: 0.5\n",
      "Eta Value: 1 K Value: 1\n",
      "Eta Value: 1 K Value: 2\n",
      "Eta Value: 1 K Value: 5\n",
      "Eta Value: 2 K Value: 0.1\n",
      "Eta Value: 2 K Value: 0.5\n",
      "Eta Value: 2 K Value: 1\n",
      "Eta Value: 2 K Value: 2\n",
      "Eta Value: 2 K Value: 5\n",
      "Eta Value: 5 K Value: 0.1\n",
      "Eta Value: 5 K Value: 0.5\n",
      "Eta Value: 5 K Value: 1\n",
      "Eta Value: 5 K Value: 2\n",
      "Eta Value: 5 K Value: 5\n"
     ]
    }
   ],
   "source": [
    "stochastic_tuning_scores = {}\n",
    "\n",
    "for eta in eta_values:\n",
    "    for k in k_values:\n",
    "        print(\"Eta Value:\", eta, \"K Value:\", k)\n",
    "        stochastic_gradient_regression = StochasticGradientRegressor(initial_learning_rate=eta, n_epochs=500, seed=42, error_threshold=1e-6, convergence_threshold=1e-6, k=k)\n",
    "        stochastic_gradient_scores = cross_validate(stochastic_gradient_regression, X_train, y_train, cv=10, isEarlyStoppingUsingVal=True)\n",
    "        stochastic_tuning_scores[(eta, k)] = stochastic_gradient_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the best parameters out of these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Best Learning Rate and K Value\n",
    "best_eta = float(\"-inf\")\n",
    "best_k = float(\"-inf\")\n",
    "min_rmse = float(\"inf\")\n",
    "std_of_min_rmse = float(\"inf\")\n",
    "\n",
    "for eta in eta_values:\n",
    "    for k in k_values:\n",
    "        if stochastic_tuning_scores[(eta, k)][\"rmse\"].mean() < min_rmse:\n",
    "            best_eta = eta\n",
    "            best_k = k\n",
    "            min_rmse = stochastic_tuning_scores[(eta, k)][\"rmse\"].mean()\n",
    "            std_of_min_rmse = stochastic_tuning_scores[(eta, k)][\"rmse\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Learning Rate: 0.01\n",
      "Best K Value: 0.1\n",
      "Minimum RMSE: 14.240562330964224\n",
      "Standard Deviation of Minimum RMSE: 1.8524472549958861\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Learning Rate:\", best_eta)\n",
    "print(\"Best K Value:\", best_k)\n",
    "print(\"Minimum RMSE:\", min_rmse)\n",
    "print(\"Standard Deviation of Minimum RMSE:\", std_of_min_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "RMSE Scores:\n",
      "*************************\n",
      "Scores: [16.95945805 17.66174809 15.16425607 13.30838709 11.8275381  14.34231345\n",
      " 14.53172087 11.86976031 13.84087845 12.89956283]\n",
      "Mean: 14.240562330964224\n",
      "Std: 1.8524472549958861\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stochastic_gradient_regression_scores = stochastic_tuning_scores[(best_eta, best_k)]\n",
    "print_scores(stochastic_gradient_regression_scores, rmse=True, mae=False, r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores['Stochastic Gradient Regression'] = stochastic_gradient_regression_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Mini-Batch Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class MiniBatchGradientRegressor:\n",
    "    \n",
    "    def __init__(self, initial_learning_rate=0.1, n_epochs=100, seed=42, error_threshold=1e-6, convergence_threshold=1e-9, k=0.1, mini_batch_size=32):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.error_threshold = error_threshold\n",
    "        self.convergence_threshold = convergence_threshold\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.k = k\n",
    "        self.seed = seed\n",
    "    \n",
    "    def calculate_gradient(self, X, y, w):\n",
    "        \"\"\"\n",
    "        Calculate the gradient of the loss function with respect to w.\n",
    "        \"\"\"\n",
    "        # Print Shapes\n",
    "        # print(X.shape, y.shape, w.shape, (X.T @ ((X @ w) - y)).shape)\n",
    "        \n",
    "        return (2 / len(y)) * X.T @ ((X @ w) - y)\n",
    "    \n",
    "    def exponential_learning_schedule(self, t):\n",
    "        \"\"\"\n",
    "        Exponential Learning Schedule.\n",
    "        \"\"\"\n",
    "        return self.initial_learning_rate * np.exp(-t * self.k)\n",
    "    \n",
    "    # X_val and y_val are the validation sets used for early stopping\n",
    "    def fit(self, X, y, X_val, y_val):\n",
    "        \n",
    "        self.d = X.shape[1]\n",
    "        \n",
    "        # Randomise an Initial Theta with seed = self.seed\n",
    "        np.random.seed(self.seed)\n",
    "        theta = np.random.randn(self.d)\n",
    "        \n",
    "        # Iterate\n",
    "        t = 0\n",
    "        flag = False\n",
    "        for _ in range(self.n_epochs):\n",
    "            \n",
    "            shuffled_indices = np.random.permutation(len(y))\n",
    "            X_shuffled = X[shuffled_indices].copy()\n",
    "            y_shuffled = y[shuffled_indices].copy()\n",
    "            \n",
    "            for i in range(0, len(y), self.mini_batch_size):\n",
    "                \n",
    "                X_batch = X_shuffled[i:i + self.mini_batch_size].copy()\n",
    "                y_batch = y_shuffled[i:i + self.mini_batch_size].copy()\n",
    "            \n",
    "                # Calculate the Gradient\n",
    "                grad = self.calculate_gradient(X_batch, y_batch, theta)\n",
    "                \n",
    "                # Calculate New Theta\n",
    "                theta_new = theta - self.exponential_learning_schedule(t=t) * grad\n",
    "                \n",
    "                # Check Convergence\n",
    "                if np.linalg.norm(theta_new - theta) < self.convergence_threshold:\n",
    "                    theta = theta_new\n",
    "                    flag = True\n",
    "                    break\n",
    "                \n",
    "                # Update Theta\n",
    "                theta = theta_new.copy()\n",
    "                t += 1\n",
    "            \n",
    "            # Check MSE for Early Stopping after each epoch\n",
    "            y_pred = X_val @ theta\n",
    "            mse = mean_squared_error(y_pred, y_val)\n",
    "            if mse < self.error_threshold:\n",
    "                theta = theta_new\n",
    "                flag = True\n",
    "                break\n",
    "            \n",
    "            # Checking for Convergence or Early Stopping\n",
    "            if flag:\n",
    "                break\n",
    "            \n",
    "        self.theta = theta.copy()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the output of a dataset X.\n",
    "        \"\"\"\n",
    "        if X.shape[1] != self.d:\n",
    "            raise ValueError(\"X has wrong number of features, expected {}, got {}.\".format(self.d, X.shape[1]))\n",
    "        return X @ self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Above Gradient Descent, We have selected an Exponential Decay Function for Learning Rate $\\eta$\n",
    "\n",
    "$$\\eta_{new} = \\eta_{initial} \\times \\exp(-k * t)$$\n",
    "\n",
    "where $k$ is the Learning Rate Decay Factor and $t$ is the Current Iteration Number\n",
    "\n",
    "Hence there are three hyper-parameters to tune:\n",
    "\n",
    "1. $\\eta$ i.e. Learning Rate\n",
    "2. $k$ i.e. Decay Rate\n",
    "3. $mini\\_batch\\_size$ i.e. Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_values = [0.01, 0.1, 0.5, 1, 2, 5]\n",
    "k_values = [0.1, 0.5, 1, 2, 5]\n",
    "mini_batch_size_values = [32, 64, 128, 256, 512, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Total of 180 Combinations of $\\eta$, $k$ and $mini\\_batch\\_size$ are possible for the Hyper-Parameter Tuning\n",
    "\n",
    "So we will apply `Randomized Grid Search` to find the best Hyper-Parameters and try for 30 Random Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta: 5.0 K: 1.0 Mini Batch Size: 256\n",
      "Eta: 0.1 K: 1.0 Mini Batch Size: 1024\n",
      "Eta: 2.0 K: 0.1 Mini Batch Size: 128\n",
      "Eta: 0.5 K: 0.1 Mini Batch Size: 1024\n",
      "Eta: 0.01 K: 5.0 Mini Batch Size: 512\n",
      "Eta: 2.0 K: 0.1 Mini Batch Size: 32\n",
      "Eta: 0.01 K: 5.0 Mini Batch Size: 256\n",
      "Eta: 5.0 K: 2.0 Mini Batch Size: 1024\n",
      "Eta: 1.0 K: 5.0 Mini Batch Size: 32\n",
      "Eta: 0.5 K: 5.0 Mini Batch Size: 256\n",
      "Eta: 2.0 K: 0.5 Mini Batch Size: 32\n",
      "Eta: 2.0 K: 1.0 Mini Batch Size: 256\n",
      "Eta: 1.0 K: 0.1 Mini Batch Size: 1024\n",
      "Eta: 0.5 K: 1.0 Mini Batch Size: 256\n",
      "Eta: 0.1 K: 5.0 Mini Batch Size: 128\n",
      "Eta: 0.5 K: 1.0 Mini Batch Size: 1024\n",
      "Eta: 0.01 K: 5.0 Mini Batch Size: 32\n",
      "Eta: 0.5 K: 5.0 Mini Batch Size: 128\n",
      "Eta: 2.0 K: 2.0 Mini Batch Size: 512\n",
      "Eta: 1.0 K: 1.0 Mini Batch Size: 512\n",
      "Eta: 5.0 K: 5.0 Mini Batch Size: 512\n",
      "Eta: 0.1 K: 0.1 Mini Batch Size: 1024\n",
      "Eta: 2.0 K: 0.1 Mini Batch Size: 1024\n",
      "Eta: 0.01 K: 0.1 Mini Batch Size: 512\n",
      "Eta: 0.01 K: 5.0 Mini Batch Size: 128\n",
      "Eta: 0.1 K: 1.0 Mini Batch Size: 256\n",
      "Eta: 0.5 K: 0.1 Mini Batch Size: 512\n",
      "Eta: 1.0 K: 1.0 Mini Batch Size: 256\n",
      "Eta: 5.0 K: 2.0 Mini Batch Size: 512\n",
      "Eta: 1.0 K: 0.5 Mini Batch Size: 32\n"
     ]
    }
   ],
   "source": [
    "minibatch_tuning_scores = {}\n",
    "\n",
    "for _ in range(30):\n",
    "    \n",
    "    eta = np.random.choice(eta_values)\n",
    "    k = np.random.choice(k_values)\n",
    "    mini_batch_size = np.random.choice(mini_batch_size_values)\n",
    "    \n",
    "    while (eta, k, mini_batch_size) in minibatch_tuning_scores:\n",
    "        eta = np.random.choice(eta_values)\n",
    "        k = np.random.choice(k_values)\n",
    "        mini_batch_size = np.random.choice(mini_batch_size_values)\n",
    "        \n",
    "    print(\"Eta:\", eta, \"K:\", k, \"Mini Batch Size:\", mini_batch_size)\n",
    "    minibatch_regressor = MiniBatchGradientRegressor(initial_learning_rate=eta, k=k, mini_batch_size=mini_batch_size, n_epochs=500)\n",
    "    # Cross Validation\n",
    "    cross_validated_scores = cross_validate(minibatch_regressor, X_train, y_train, cv=10, isEarlyStoppingUsingVal=True)\n",
    "    minibatch_tuning_scores[(eta, k, mini_batch_size)] = cross_validated_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the best parameters out of these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Best Learning Rate and K Value\n",
    "best_eta = float(\"-inf\")\n",
    "best_k = float(\"-inf\")\n",
    "best_mini_batch_size = float(\"-inf\")\n",
    "min_rmse = float(\"inf\")\n",
    "std_of_min_rmse = float(\"inf\")\n",
    "\n",
    "for (eta, k, mbs) in minibatch_tuning_scores:\n",
    "    if minibatch_tuning_scores[(eta, k, mbs)][\"rmse\"].mean() < min_rmse:\n",
    "        min_rmse = minibatch_tuning_scores[(eta, k, mbs)][\"rmse\"].mean()\n",
    "        std_of_min_rmse = minibatch_tuning_scores[(eta, k, mbs)][\"rmse\"].std()\n",
    "        best_eta = eta\n",
    "        best_k = k\n",
    "        best_mini_batch_size = mbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Learning Rate: 0.1\n",
      "Best K Value: 0.1\n",
      "Best Mini Batch Size: 1024\n",
      "Minimum RMSE: 5.228997375114249\n",
      "Standard Deviation of Minimum RMSE: 0.11715114863382442\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Learning Rate:\", best_eta)\n",
    "print(\"Best K Value:\", best_k)\n",
    "print(\"Best Mini Batch Size:\", best_mini_batch_size)\n",
    "print(\"Minimum RMSE:\", min_rmse)\n",
    "print(\"Standard Deviation of Minimum RMSE:\", std_of_min_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "RMSE Scores:\n",
      "*************************\n",
      "Scores: [5.41859187 5.29402914 5.10223908 5.11786904 5.08712505 5.34181552\n",
      " 5.24732494 5.34933577 5.08521063 5.24643272]\n",
      "Mean: 5.228997375114249\n",
      "Std: 0.11715114863382442\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "minibatch_regression_scores = minibatch_tuning_scores[(best_eta, best_k, best_mini_batch_size)]\n",
    "print_scores(minibatch_regression_scores, rmse=True, mae=False, r2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores[\"Mini-Batch Regression\"] = minibatch_regression_scores.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Trying Non-Parameteric Approach (K-Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    \n",
    "    def __init__(self, k=3, merge_function='mean', distances='eucledian'):\n",
    "        self.k = k\n",
    "        self.distances = distances\n",
    "        self.merge_function = merge_function\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X = X.copy()\n",
    "        self.y = y.copy()\n",
    "        self.d = X.shape[1]\n",
    "        return self\n",
    "\n",
    "    def cosine_distance(self, x1, x2):\n",
    "        return 1 - (np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2)))\n",
    "    \n",
    "    def eucledian_distance(self, x1, x2):\n",
    "        return np.linalg.norm(x1 - x2)\n",
    "    \n",
    "    def merge(self, X):\n",
    "        # X -> [(Distance, Y)]\n",
    "        if self.merge_function == 'mean':\n",
    "            # Get a mean of all the Y values\n",
    "            return np.mean(np.array([x[1] for x in X]), axis=0)        \n",
    "        if self.merge_function == 'median':\n",
    "            # Get a median of all the Y values\n",
    "            return np.median(np.array([x[1] for x in X]), axis=0)\n",
    "        # Naive Weighted Average\n",
    "        if self.merge_function == 'weighted_mean':\n",
    "            wm = 0\n",
    "            w_sum = 0\n",
    "            for (dist, y) in X:\n",
    "                wm += (np.exp(-dist)) * y\n",
    "                w_sum += (np.exp(-dist))\n",
    "            return wm / w_sum\n",
    "        \n",
    "        raise ValueError(\"Invalid merge function.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Get Nearest K\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            all_neighbours = []            \n",
    "            for i in range(len(self.X)):\n",
    "                # (Distance, Y Value)\n",
    "                if self.distances == 'cosine':\n",
    "                    all_neighbours.append((self.cosine_distance(x, self.X[i]), self.y[i]))\n",
    "                elif self.distances == 'eucledian':\n",
    "                    all_neighbours.append((self.eucledian_distance(x, self.X[i]), self.y[i]))\n",
    "            # Sort by Distance\n",
    "            all_neighbours.sort(key=lambda x: x[0])\n",
    "            # Get K Nearest Neighbours\n",
    "            k_nearest_neighbours = all_neighbours[:self.k]\n",
    "            # Predict Output based on these\n",
    "            predictions.append(self.merge(k_nearest_neighbours))\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our KNN Approach, we have the following Hyper-Parameters:\n",
    "\n",
    "1. $k$ i.e. Number of Neighbors\n",
    "2. $merge\\_function$ i.e. Kernel Function which will be used to compute the answer from the the Top-K Neighbor Values\n",
    "3. $distances$ i.e. Distance Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the different ***Merge Functions***:\n",
    "\n",
    "1. **Mean** - Average of the Top-K Neighbor Values\n",
    "2. **Median** - Median of the Top-K Neighbor Values\n",
    "3. **Weighted Mean** - Weighted Average of the Top-K Neighbor Values where Weights are defined as $exp(-d) and $d$ is the distance of that datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the different ***Distance*** Metrics:\n",
    "\n",
    "1. **Euclidean Distance** : $  d(x,y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \\cdots + (x_d - y_d)^2}$\n",
    "2. **Cosine Distance** i.e. **1 - Cosine Similarity** : $d(x,y) = 1 - \\frac{\\sum_{i=1}^{d}x_i y_i}{\\sqrt{\\sum_{i=1}^{d}x_i^2} \\sqrt{\\sum_{i=1}^{d}y_i^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = ['cosine', 'eucledian']\n",
    "merge_functions = ['mean', 'median', 'weighted_mean', 'e_kernel', 'threshold_kernel']\n",
    "k_values = [1, 3, 5, 7, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in distances:\n",
    "    for kernel in merge_functions:\n",
    "        for k in k_values:\n",
    "            print(\"KNN:\", d, kernel, k)\n",
    "            knn = KNN(k=k, merge_function=kernel, distances=d)\n",
    "            knn.fit(X_train, y_train)\n",
    "            knn_scores = cross_validate(knn, X_train, y_train, cv=10, isEarlyStoppingUsingVal=False)\n",
    "            print_scores(knn_scores, rmse=True, mae=False, r2=False)\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was taking alot of time even for a single combination of parameters. Hence, I am opting for Skleans's KNN Implementation as it is much more optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Implementation using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNN Regressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-Parameters:\n",
    "\n",
    "1. Number of Neighbours\n",
    "2. Weights: 'uniform', 'distance' or a callable function\n",
    "3. p: Power parameter for the Minkowski metric (p = 1 for Manhattan distance and p = 2 for Euclidean distance)\n",
    "4. metric: The distance metric to use for the tree. \n",
    "    - 'minkowski' : use the Minkowski metric (the default). Here, p=1 for Manhattan Distance and p=2 for Euclidean Distance.\n",
    "    - 'cosine' : use the cosine metric.\n",
    "    - 'haversine' : use the haversine metric.\n",
    "    - 'cityblock' : use the city block metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_kernel(distances):\n",
    "    return np.exp(-distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neighbours = [1, 3, 5, 7, 9]\n",
    "weights = ['uniform', e_kernel]\n",
    "p = [1, 2]\n",
    "metric = ['minkowski', 'cosine', 'haversine', 'cityblock']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this set of Hyper-Parameters, we have 80 possible Combinations.\n",
    "\n",
    "Hence, we will again use `Randomized Grid Search` to find the best Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rmse = float(\"inf\")\n",
    "best_num_neighbours = 0\n",
    "best_weight = \"\"\n",
    "best_p = 0\n",
    "best_metric = \"\"\n",
    "knn_tuning_scores = {}\n",
    "\n",
    "for _ in range(30):\n",
    "    \n",
    "    # Randomly select a number of neighbours, weight and metric\n",
    "    n = np.random.choice(num_neighbours)\n",
    "    w = np.random.choice(weights)\n",
    "    m = np.random.choice(metric)\n",
    "    p = np.random.choice(p)\n",
    "    \n",
    "    while (n, w, m, p) in knn_tuning_scores:\n",
    "        n = np.random.choice(num_neighbours)\n",
    "        w = np.random.choice(weights)\n",
    "        m = np.random.choice(metric)\n",
    "        p = np.random.choice(p)\n",
    "    \n",
    "    print(\"KNN:\", n, w, m, p)\n",
    "    \n",
    "    knn = KNeighborsRegressor(n_neighbors=n, weights=w, metric=m, p=p)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_scores = cross_validate(knn, X_train, y_train, cv=10, isEarlyStoppingUsingVal=False)\n",
    "    if knn_scores[\"rmse\"].mean() < min_rmse:\n",
    "        min_rmse = knn_scores[\"rmse\"].mean()\n",
    "        best_num_neighbours = n\n",
    "        best_weight = w\n",
    "        best_p = p\n",
    "        best_metric = m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even this is taking so much time.... :(\n",
    "\n",
    "Hence, we could not perform K-Nearset Neighbors Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Best Models for Matrix Based and Optimization Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Linear Regression: 4.816662676483373\n",
      "Pseudo Inverse Linear Regression: 4.816662676483373\n",
      "QR Linear Regression: 4.816662676483373\n",
      "Sklearn Simple Linear Regression: 4.816662676483373\n",
      "Sklearn Ridge Regression: 4.816662554391599\n",
      "Sklearn Lasso Regression: 4.816701142384302\n",
      "Sklearn Elastic Net Regression: 4.81661063548957\n",
      "Ridge Regression Manual: 4.816662554391599\n",
      "Batch Gradient Regression: 4.815670541368664\n",
      "Stochastic Gradient Regression: 14.240562330964224\n",
      "Mini-Batch Regression: 5.228997375114249\n"
     ]
    }
   ],
   "source": [
    "for model in total_scores:\n",
    "    \n",
    "    print(model, end=': ')\n",
    "    # Print Average RMSE\n",
    "    print(total_scores[model][\"rmse\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, in Terms of Accuracy:\n",
    "\n",
    "- For Matrix Based Models, we have found the best model is Ridge Regression\n",
    "- For Optimization Based Models, we have found the best model is Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training these Models on Complete Training Data and Testing on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model-1 : Ridge Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Hyper-Parameter Tuning, we have found the best value for $\\lambda$ is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on Complete Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_linear_regression = RidgeRegression(lambda_=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RidgeRegression at 0x13d4f9cc0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_linear_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ridge_linear_regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Score on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Test Data:  4.791831611358817\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"RMSE for Test Data: \", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the Final Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.34005581, -1.03627812, -0.42061303,  0.97007229,  6.84581034,\n",
       "       -1.62324499,  0.45000422,  0.93558349, -1.11857759,  1.5537534 ,\n",
       "        0.82385253,  0.34212281,  0.85203588,  1.15565897,  0.64775276,\n",
       "       -0.16681822,  0.17252397, -0.20962758,  0.13267107])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_linear_regression.w_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_order = feature_order = ['1', 'pickup_longitude', 'dropoff_longitude', 'year',\n",
    "       'H_Distance', 'jfk_Distance_Pickup', 'jfk_Distance_Dropoff',\n",
    "       'sol_Distance_Dropoff', 'cp_Distance_Pickup', 'cp_Distance_Dropoff',\n",
    "       'pickup_near_jfk', 'dropoff_near_jfk', 'dropoff_near_ewr',\n",
    "       'pickup_near_lga', 'dropoff_near_lga', 'pickup_near_esm',\n",
    "       'dropoff_near_esm', 'pickup_near_ts', 'dropoff_near_ts']\n",
    "coefficients = list(ridge_linear_regression.w_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equation of the model: \n",
      "y = 11.340055810742994 * 1  +\n",
      "-1.036278122588903 * pickup_longitude  +\n",
      "-0.4206130339446861 * dropoff_longitude  +\n",
      "0.9700722933149348 * year  +\n",
      "6.8458103387280795 * H_Distance  +\n",
      "-1.6232449865635599 * jfk_Distance_Pickup  +\n",
      "0.45000422080251173 * jfk_Distance_Dropoff  +\n",
      "0.9355834900195628 * sol_Distance_Dropoff  +\n",
      "-1.1185775883531568 * cp_Distance_Pickup  +\n",
      "1.553753397052487 * cp_Distance_Dropoff  +\n",
      "0.8238525341050202 * pickup_near_jfk  +\n",
      "0.3421228108021715 * dropoff_near_jfk  +\n",
      "0.8520358752562225 * dropoff_near_ewr  +\n",
      "1.1556589656135354 * pickup_near_lga  +\n",
      "0.6477527615076163 * dropoff_near_lga  +\n",
      "-0.16681821850270015 * pickup_near_esm  +\n",
      "0.17252397346686255 * dropoff_near_esm  +\n",
      "-0.20962757928396752 * pickup_near_ts  +\n",
      "0.13267106888614766 * dropoff_near_ts  "
     ]
    }
   ],
   "source": [
    "# Print the Equation of the model\n",
    "print(\"Equation of the model: \")\n",
    "\n",
    "print(\"y = \", end=\"\")\n",
    "for i in range(len(coefficients)):\n",
    "\n",
    "    print(coefficients[i], end=\" \")\n",
    "    print(\"*\", end=\" \")\n",
    "    print(feature_order[i], end=\" \")\n",
    "    print(\" \", end=\"\")\n",
    "\n",
    "    if i < len(coefficients) - 1:\n",
    "        print(\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Cleaner Equation with Coefficients Rounded to 2 decimal Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equation of the model: \n",
      "y = (11.34 * 1) + (-1.04 * pickup_longitude) + (-0.42 * dropoff_longitude) + (0.97 * year) + (6.85 * H_Distance) + (-1.62 * jfk_Distance_Pickup) + (0.45 * jfk_Distance_Dropoff) + (0.94 * sol_Distance_Dropoff) + (-1.12 * cp_Distance_Pickup) + (1.55 * cp_Distance_Dropoff) + (0.82 * pickup_near_jfk) + (0.34 * dropoff_near_jfk) + (0.85 * dropoff_near_ewr) + (1.16 * pickup_near_lga) + (0.65 * dropoff_near_lga) + (-0.17 * pickup_near_esm) + (0.17 * dropoff_near_esm) + (-0.21 * pickup_near_ts) + (0.13 * dropoff_near_ts) "
     ]
    }
   ],
   "source": [
    "# Print the Equation of the model\n",
    "print(\"Equation of the model: \")\n",
    "\n",
    "print(\"y = \", end=\"\")\n",
    "for i in range(len(coefficients)):\n",
    "\n",
    "    print(\"(\", end=\"\")\n",
    "    print(f\"{coefficients[i]:.2f}\", end=\" \")\n",
    "    print(\"*\", end=\" \")\n",
    "    print(feature_order[i], end=\")\")\n",
    "    print(\" \", end=\"\")\n",
    "\n",
    "    if i < len(coefficients) - 1:\n",
    "        print(\"+\", end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model-2 : Batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the Hyper-Parameter Tuning, we have found that the best hyper-parameter is $\\eta = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gradient_descent = BatchGradientRegressor(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Fit Batch Gradient Descent, we need a Validation set.\n",
    "\n",
    "I will take 10% of the Training Data as Validation Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_batch, X_val, y_train_batch, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.BatchGradientRegressor at 0x13d4fb3d0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gradient_descent.fit(X_train_batch, y_train_batch, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = batch_gradient_descent.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Score on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Test Data:  4.792088387469097\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE for Test Data: \", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.3371823 , -1.05652291, -0.33597433,  0.97033492,  6.81574329,\n",
       "       -1.65588789,  0.49787812,  0.87921817, -1.14239719,  1.55883173,\n",
       "        0.83717273,  0.36134181,  0.86691597,  1.16016398,  0.64300068,\n",
       "       -0.17003514,  0.16860468, -0.21406117,  0.13670618])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gradient_descent.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the Final Equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_order = ['1', 'pickup_longitude', 'dropoff_longitude', 'year',\n",
    "       'H_Distance', 'jfk_Distance_Pickup', 'jfk_Distance_Dropoff',\n",
    "       'sol_Distance_Dropoff', 'cp_Distance_Pickup', 'cp_Distance_Dropoff',\n",
    "       'pickup_near_jfk', 'dropoff_near_jfk', 'dropoff_near_ewr',\n",
    "       'pickup_near_lga', 'dropoff_near_lga', 'pickup_near_esm',\n",
    "       'dropoff_near_esm', 'pickup_near_ts', 'dropoff_near_ts']\n",
    "coefficients = list(batch_gradient_descent.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equation of the model: \n",
      "y = 11.33718230479152 * 1  +\n",
      "-1.0565229134175749 * pickup_longitude  +\n",
      "-0.33597432731010735 * dropoff_longitude  +\n",
      "0.9703349167862033 * year  +\n",
      "6.815743285499751 * H_Distance  +\n",
      "-1.6558878871648717 * jfk_Distance_Pickup  +\n",
      "0.4978781188180685 * jfk_Distance_Dropoff  +\n",
      "0.8792181746704661 * sol_Distance_Dropoff  +\n",
      "-1.1423971914651652 * cp_Distance_Pickup  +\n",
      "1.5588317333690476 * cp_Distance_Dropoff  +\n",
      "0.8371727292396821 * pickup_near_jfk  +\n",
      "0.36134181159893813 * dropoff_near_jfk  +\n",
      "0.8669159682471761 * dropoff_near_ewr  +\n",
      "1.1601639787398805 * pickup_near_lga  +\n",
      "0.643000683784307 * dropoff_near_lga  +\n",
      "-0.1700351402667336 * pickup_near_esm  +\n",
      "0.16860468257256525 * dropoff_near_esm  +\n",
      "-0.21406116853396293 * pickup_near_ts  +\n",
      "0.13670618328110823 * dropoff_near_ts  "
     ]
    }
   ],
   "source": [
    "# Print the Equation of the model\n",
    "print(\"Equation of the model: \")\n",
    "\n",
    "print(\"y = \", end=\"\")\n",
    "for i in range(len(coefficients)):\n",
    "\n",
    "    print(coefficients[i], end=\" \")\n",
    "    print(\"*\", end=\" \")\n",
    "    print(feature_order[i], end=\" \")\n",
    "    print(\" \", end=\"\")\n",
    "\n",
    "    if i < len(coefficients) - 1:\n",
    "        print(\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Cleaner Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equation of the model: \n",
      "y = (11.34 * 1) + (-1.06 * pickup_longitude) + (-0.34 * dropoff_longitude) + (0.97 * year) + (6.82 * H_Distance) + (-1.66 * jfk_Distance_Pickup) + (0.50 * jfk_Distance_Dropoff) + (0.88 * sol_Distance_Dropoff) + (-1.14 * cp_Distance_Pickup) + (1.56 * cp_Distance_Dropoff) + (0.84 * pickup_near_jfk) + (0.36 * dropoff_near_jfk) + (0.87 * dropoff_near_ewr) + (1.16 * pickup_near_lga) + (0.64 * dropoff_near_lga) + (-0.17 * pickup_near_esm) + (0.17 * dropoff_near_esm) + (-0.21 * pickup_near_ts) + (0.14 * dropoff_near_ts) "
     ]
    }
   ],
   "source": [
    "# Print the Equation of the model\n",
    "print(\"Equation of the model: \")\n",
    "\n",
    "print(\"y = \", end=\"\")\n",
    "for i in range(len(coefficients)):\n",
    "\n",
    "    print(\"(\", end=\"\")\n",
    "    print(f\"{coefficients[i]:.2f}\", end=\" \")\n",
    "    print(\"*\", end=\" \")\n",
    "    print(feature_order[i], end=\")\")\n",
    "    print(\" \", end=\"\")\n",
    "\n",
    "    if i < len(coefficients) - 1:\n",
    "        print(\"+\", end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Which model is the best for our problem?***\n",
    "\n",
    "First, all the approaches are giving almost similar results in terms of Accuracy. All the approaches are giving a Mean RMSE of around 4.7 except Scholastic Gradient Descent but that is because of its unstable nature and poor convergence.\n",
    "\n",
    "Other than that, any approach can outperform any other approach if we tweak with parameters or change the random seeds or increase/decrease data.\n",
    "\n",
    "Hence, we need to compare all the approaches on some other factor.\n",
    "\n",
    "If we compare them on the basis of efficiency,\n",
    "\n",
    "Then definitely optimization based models beat Matrix Based and Non-Parameteric\n",
    "\n",
    "Since, as we increase the number of rows in our data, Matrix based models will become very slow because of Inverse Operation and Non-Parameteric (KNN) model will require alot of space and will take lots of time in prediction.\n",
    "\n",
    "Amongst the Optimization based models, clearly Batch Gradient Descent will require lots of memory if we increase data, hence the competition is between Scholastic Gradient Descent and Mini Batch Gradient Descent.\n",
    "\n",
    "Amongst those two, Mini-Batch Gradient Descent would be better because of faster and more stable convergence.\n",
    "\n",
    "Also, if we increase the data, then convergence times can be controlled by increasing initial learning rate and controlling the decay properly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e36d9a1e913ec06bfe391120a807be4aea3e06fb814dd30b6cd496bf8ea7706"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
