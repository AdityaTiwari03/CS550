{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2022-12-02T10:33:43.339557Z",
     "shell.execute_reply": "2022-12-02T10:33:43.338723Z",
     "shell.execute_reply.started": "2022-12-02T10:33:40.910916Z"
    }
   },
   "outputs": [],
   "source": [
    "#  import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, Input, concatenate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Conv2DTranspose, UpSampling2D, MaxPooling2D\n",
    "\n",
    "#  import mse loss function\n",
    "from tensorflow.keras.losses import mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an autoencoder?\n",
    "\n",
    "An autoencoder is a type of neural network that is used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. \n",
    "\n",
    "Autoencoders are data-specific, which means that if you train an autoencoder on one set of data, then it will not necessarily be able to reduce any other data. For this reason, they are most useful as a pre-processing step for another machine learning algorithm rather than as a standalone model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STACKED AUTOENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a stacked autoencoder?\n",
    "\n",
    "A stacked autoencoder have multiple hidden layers. Adding more layers to the autoencoder allows it to learn more complex representations of the input data. The first hidden layer of the autoencoder learns to extract the most basic features of the input data, and each subsequent hidden layer builds on the features learned by the previous layers. The architecture of a stacked autoencoder is symmetrical with regards to the central hidden layer. It looks like a sandwitch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of a stacked autoencoder is typically symmetrical with regards to the\n",
    "central hidden layer (the coding layer). To put it simply, it looks like a sandwich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T10:33:43.341600Z",
     "iopub.status.busy": "2022-12-02T10:33:43.340896Z",
     "iopub.status.idle": "2022-12-02T10:33:43.344715Z",
     "shell.execute_reply": "2022-12-02T10:33:43.343998Z",
     "shell.execute_reply.started": "2022-12-02T10:33:43.341575Z"
    }
   },
   "outputs": [],
   "source": [
    "#  import regularizer for the convolutional layers\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T10:33:43.352014Z",
     "iopub.status.busy": "2022-12-02T10:33:43.351820Z",
     "iopub.status.idle": "2022-12-02T10:33:43.359747Z"
    }
   },
   "outputs": [],
   "source": [
    "class StackedAutoEncoder:\n",
    "\n",
    "    def __init__(self, input_dim, batch_size = 32, learning_rate = 1e-5):\n",
    "        self.input_dim = input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        #  input layer \n",
    "        x = Input(shape=(self.input_dim, self.input_dim, 1), name='encoder_input')\n",
    "        # Input layer is 28x28x1 (wide and high)\n",
    "\n",
    "        #  encoder\n",
    "        #  convolutional layer\n",
    "        conv1 = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "        #  convolutional layer\n",
    "        conv2 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(conv1)\n",
    "        #  max pooling layer\n",
    "        pool1 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
    "        #  convolutional layer\n",
    "        conv3 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(pool1)\n",
    "        #  convolutional layer\n",
    "        conv4 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(conv3)\n",
    "        #  max pooling layer\n",
    "        pool2 = MaxPool2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "\n",
    "        #  decoder\n",
    "        #  upsampling layer\n",
    "        up1 = UpSampling2D(size=(2, 2))(pool2)\n",
    "        #  convolutional layer\n",
    "        conv7 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(up1)\n",
    "        #  convolutional layer\n",
    "        conv8 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(conv7)\n",
    "        #  upsampling layer\n",
    "        up2 = UpSampling2D(size=(2, 2))(conv8)\n",
    "        #  convolutional layer\n",
    "        conv9 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(up2)\n",
    "        #  convolutional layer\n",
    "        conv10 = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "        #  output layer\n",
    "        output = Conv2D(filters=1, kernel_size=(3, 3), activation='sigmoid', padding='same')(conv10)\n",
    "\n",
    "        self.model = Model(x, output)\n",
    "\n",
    "        self.model.compile(optimizer=Adam(lr=self.learning_rate), loss='mse')\n",
    "\n",
    "        self.model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model and Testing Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T10:33:43.360864Z",
     "iopub.status.busy": "2022-12-02T10:33:43.360563Z",
     "iopub.status.idle": "2022-12-02T10:33:43.785021Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T10:33:43.786234Z",
     "iopub.status.busy": "2022-12-02T10:33:43.785854Z",
     "iopub.status.idle": "2022-12-02T10:33:44.099830Z"
    }
   },
   "outputs": [],
   "source": [
    "# data mnsit \n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T10:33:44.101173Z",
     "iopub.status.busy": "2022-12-02T10:33:44.100966Z",
     "iopub.status.idle": "2022-12-02T10:33:44.104980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# shape of the data\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T10:33:44.106562Z",
     "iopub.status.busy": "2022-12-02T10:33:44.106053Z",
     "iopub.status.idle": "2022-12-02T10:33:44.240041Z"
    }
   },
   "outputs": [],
   "source": [
    "#  stratify sample 6000 images from the training set and 100 images from the test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42, stratify=y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the Stacked Autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T10:33:44.241073Z",
     "iopub.status.busy": "2022-12-02T10:33:44.240856Z",
     "iopub.status.idle": "2022-12-02T10:33:45.398147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 14, 14, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 28, 28, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 32)        18464     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 28, 28, 1)         289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222,209\n",
      "Trainable params: 222,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#  stack autoencoder\n",
    "sae = StackedAutoEncoder(input_dim=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Stacked Autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T10:33:45.399231Z",
     "iopub.status.busy": "2022-12-02T10:33:45.398988Z",
     "iopub.status.idle": "2022-12-02T10:38:09.365611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1688/1688 [==============================] - 12s 5ms/step - loss: 0.0480 - val_loss: 0.0091\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0069 - val_loss: 0.0056\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0048 - val_loss: 0.0042\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.0011 - val_loss: 0.0011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f97d531caf0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  train the model\n",
    "sae.model.fit(x_train, x_train, epochs=30, validation_data=(x_val, x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T10:38:09.366937Z",
     "iopub.status.busy": "2022-12-02T10:38:09.366326Z",
     "iopub.status.idle": "2022-12-02T10:38:10.211950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0011129977647215128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the models\n",
    "sae.model.evaluate(x_test, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T10:38:10.214410Z",
     "iopub.status.busy": "2022-12-02T10:38:10.213690Z",
     "iopub.status.idle": "2022-12-02T10:38:10.218232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the model on random image input\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test[0:1]\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-02T10:38:10.219591Z",
     "iopub.status.busy": "2022-12-02T10:38:10.218875Z",
     "iopub.status.idle": "2022-12-02T10:38:10.539089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 152ms/step\n",
      "Input\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from Stacked Auto Encoder\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  predict the output\n",
    "x_pred = sae.model.predict(x_test)\n",
    "\n",
    " # plot the input and output\n",
    "plt.figure(figsize=(20, 4))\n",
    "print(\"Input\")\n",
    "for i in range(0,1):\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "print(\"Output from Stacked Auto Encoder\")\n",
    "for i in range(0,1):\n",
    "    plt.imshow(x_pred[i].reshape(28, 28))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7036f2c589e66350e558e75aef7cf6e2c2eca72d9af89a4326c4f1191cd6de16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
